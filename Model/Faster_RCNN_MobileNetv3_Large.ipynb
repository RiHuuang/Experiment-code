{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5WsB1lYa8Wa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import shutil\n",
        "import yaml\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# Install specific versions for stability if not present\n",
        "try:\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "except ImportError:\n",
        "    print(\"Installing dependencies...\")\n",
        "    os.system(\"pip install -q albumentations==1.4.0 torchmetrics\")\n",
        "    import albumentations as A\n",
        "    from albumentations.pytorch import ToTensorV2\n",
        "    from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Ensure Deterministic Behavior (Reproducibility)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Configuration\n",
        "BASE_SAVE_DIR = \"/content/drive/MyDrive/Model/FasterRCNN_Large\"\n",
        "DRIVE_YAML_PATH = \"/content/drive/MyDrive/Dataset/FINAL_YOLO_SPLIT/dataset.yaml\"\n",
        "LOCAL_DATA_DIR = \"/content/local_dataset\"\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "print(f\"Computation Device: {DEVICE}\")\n",
        "os.makedirs(BASE_SAVE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not os.path.exists(LOCAL_DATA_DIR):\n",
        "    print(f\"Copying dataset from Drive to {LOCAL_DATA_DIR}...\")\n",
        "    try:\n",
        "        drive_data_dir = os.path.dirname(DRIVE_YAML_PATH)\n",
        "        shutil.copytree(drive_data_dir, LOCAL_DATA_DIR)\n",
        "        print(\"Dataset copy complete.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying dataset: {e}\")\n",
        "else:\n",
        "    print(f\"Local dataset found at {LOCAL_DATA_DIR}. Skipping copy.\")\n",
        "\n",
        "# Class Configuration\n",
        "# Mapping: YOLO ID (from .txt) -> Model ID\n",
        "# YOLO: 0=Brain (Ignore), 1=CSP, 2=LV\n",
        "# Model: 0=Background, 1=CSP, 2=LV\n",
        "CLASS_NAMES = ['CSP', 'LV']\n",
        "NUM_CLASSES = len(CLASS_NAMES) + 1\n",
        "TARGET_MAPPING = {1: 1, 2: 2}\n",
        "\n",
        "print(f\"Classes: {CLASS_NAMES}\")"
      ],
      "metadata": {
        "id": "1V9Igk4AbAo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLODataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transforms=None, mapping=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transforms = transforms\n",
        "        self.target_mapping = mapping if mapping else {1: 1, 2: 2}\n",
        "\n",
        "        self.img_dir = os.path.join(root_dir, split, 'images')\n",
        "        self.label_dir = os.path.join(root_dir, split, 'labels')\n",
        "\n",
        "        # Load all valid image files\n",
        "        self.img_files = sorted(glob.glob(os.path.join(self.img_dir, \"*.jpg\")) +\n",
        "                                glob.glob(os.path.join(self.img_dir, \"*.png\")))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_files[idx]\n",
        "        file_name = os.path.basename(img_path)\n",
        "        label_file = os.path.splitext(file_name)[0] + \".txt\"\n",
        "        label_path = os.path.join(self.label_dir, label_file)\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            return self.__getitem__((idx + 1) % len(self.img_files))\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        h, w, _ = image.shape\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        # Parse YOLO Labels\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            for line in lines:\n",
        "                parts = list(map(float, line.strip().split()))\n",
        "                cls_id_raw = int(parts[0])\n",
        "\n",
        "                if cls_id_raw in self.target_mapping:\n",
        "                    final_cls_id = self.target_mapping[cls_id_raw]\n",
        "                    x_c, y_c, bw, bh = parts[1], parts[2], parts[3], parts[4]\n",
        "\n",
        "                    # Convert xywh (normalized) to xyxy (absolute)\n",
        "                    x_min = (x_c - bw / 2) * w\n",
        "                    y_min = (y_c - bh / 2) * h\n",
        "                    x_max = (x_c + bw / 2) * w\n",
        "                    y_max = (y_c + bh / 2) * h\n",
        "\n",
        "                    # Clip to image boundaries\n",
        "                    x_min = max(0, x_min)\n",
        "                    y_min = max(0, y_min)\n",
        "                    x_max = min(w, x_max)\n",
        "                    y_max = min(h, y_max)\n",
        "\n",
        "                    if x_max > x_min and y_max > y_min:\n",
        "                        boxes.append([x_min, y_min, x_max, y_max])\n",
        "                        labels.append(final_cls_id)\n",
        "\n",
        "        # Convert to Tensor\n",
        "        if len(boxes) > 0:\n",
        "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        # Apply Transforms\n",
        "        if self.transforms:\n",
        "            try:\n",
        "                transformed = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
        "                image = transformed['image']\n",
        "                boxes = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "                labels = torch.as_tensor(transformed['labels'], dtype=torch.int64)\n",
        "            except Exception:\n",
        "                # If augmentation fails (rare geometry error), return original\n",
        "                image = ToTensorV2()(image=image)[\"image\"]\n",
        "\n",
        "        else:\n",
        "            image = ToTensorV2()(image=image)[\"image\"]\n",
        "\n",
        "        # Final check for empty boxes after transform\n",
        "        if len(boxes) == 0:\n",
        "             boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "             labels = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = torch.tensor([idx])\n",
        "\n",
        "        # Calculate Area (required for COCO evaluation)\n",
        "        if len(boxes) > 0:\n",
        "            target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "            target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "        else:\n",
        "            target[\"area\"] = torch.as_tensor([], dtype=torch.float32)\n",
        "            target[\"iscrowd\"] = torch.as_tensor([], dtype=torch.int64)\n",
        "\n",
        "        # Normalize Image (0-255 to 0-1)\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.dtype == torch.uint8:\n",
        "                image = image.float() / 255.0\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def get_transforms(condition='Raw'):\n",
        "    \"\"\"\n",
        "    Returns Albumentations composition.\n",
        "    Raw: Resize + ToTensor\n",
        "    Tuned: Resize + Geometric Augmentations + ToTensor\n",
        "    \"\"\"\n",
        "    bbox_params = A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.1)\n",
        "\n",
        "    base_ops = [\n",
        "        A.Resize(height=640, width=640),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        "\n",
        "    if condition == 'Tuned':\n",
        "        aug_ops = [\n",
        "            A.Affine(scale=(0.6, 1.4), translate_percent=(0, 0.2), rotate=(-45, 45), shear=(-5, 5), p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Perspective(scale=(0.05, 0.1), p=0.5)\n",
        "        ]\n",
        "        return A.Compose(aug_ops + base_ops, bbox_params=bbox_params)\n",
        "    else:\n",
        "        return A.Compose(base_ops, bbox_params=bbox_params)"
      ],
      "metadata": {
        "id": "MCgZkVbrbHyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_mobilenetv3_large(num_classes):\n",
        "    weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
        "    model = fasterrcnn_mobilenet_v3_large_fpn(weights=weights)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_test = get_model_mobilenetv3_large(NUM_CLASSES)\n",
        "    print(f\"Model Architecture Ready: Faster R-CNN MobileNetV3-Large\")"
      ],
      "metadata": {
        "id": "IhHbIMzBbRoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "OUa086PMcYKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_map_complete(model, dataloader, device):\n",
        "    \"\"\"Calculates mAP 50-95 (Global) and mAP 50 (Per Class).\"\"\"\n",
        "    model.eval()\n",
        "    metric_global = MeanAveragePrecision(class_metrics=True).to(device)\n",
        "    metric_50 = MeanAveragePrecision(class_metrics=True, iou_thresholds=[0.5]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            t_clean = [{k: v.to(device) for k, v in t.items() if k in ['boxes', 'labels']} for t in targets]\n",
        "\n",
        "            outputs = model(images)\n",
        "            metric_global.update(outputs, t_clean)\n",
        "            metric_50.update(outputs, t_clean)\n",
        "\n",
        "    res_global = metric_global.compute()\n",
        "    res_50 = metric_50.compute()\n",
        "\n",
        "    return {\n",
        "        'map': res_global['map'].item(),\n",
        "        'map_50': res_global['map_50'].item(),\n",
        "        'map_per_class': res_global['map_per_class'],\n",
        "        'map_50_per_class': res_50['map_per_class']\n",
        "    }\n",
        "\n",
        "def evaluate_best_f1(model, dataloader, device, num_classes):\n",
        "    \"\"\"Calculates Best F1-Score, Precision, and Recall.\"\"\"\n",
        "    model.eval()\n",
        "    class_preds = {i: [] for i in range(1, num_classes)}\n",
        "    class_gt_counts = {i: 0 for i in range(1, num_classes)}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i, output in enumerate(outputs):\n",
        "                pred_boxes = output['boxes']\n",
        "                pred_scores = output['scores']\n",
        "                pred_labels = output['labels']\n",
        "                gt_boxes = targets[i]['boxes'].to(device)\n",
        "                gt_labels = targets[i]['labels'].to(device)\n",
        "\n",
        "                for cls_id in range(1, num_classes):\n",
        "                    class_gt_counts[cls_id] += (gt_labels == cls_id).sum().item()\n",
        "\n",
        "                if len(pred_scores) == 0: continue\n",
        "\n",
        "                # Sort predictions by confidence\n",
        "                sorted_indices = torch.argsort(pred_scores, descending=True)\n",
        "                pred_boxes = pred_boxes[sorted_indices]\n",
        "                pred_scores = pred_scores[sorted_indices]\n",
        "                pred_labels = pred_labels[sorted_indices]\n",
        "\n",
        "                iou_matrix = torchvision.ops.box_iou(pred_boxes, gt_boxes) if len(gt_boxes) > 0 else None\n",
        "                used_gt_indices = set()\n",
        "\n",
        "                for p_idx in range(len(pred_boxes)):\n",
        "                    p_label = pred_labels[p_idx].item()\n",
        "                    p_score = pred_scores[p_idx].item()\n",
        "\n",
        "                    is_tp = False\n",
        "                    if iou_matrix is not None:\n",
        "                        ious = iou_matrix[p_idx]\n",
        "                        if len(ious) > 0:\n",
        "                            max_iou, max_gt_idx = torch.max(ious, dim=0)\n",
        "                            if max_iou > 0.5 and gt_labels[max_gt_idx].item() == p_label and max_gt_idx.item() not in used_gt_indices:\n",
        "                                is_tp = True\n",
        "                                used_gt_indices.add(max_gt_idx.item())\n",
        "\n",
        "                    class_preds[p_label].append((p_score, is_tp))\n",
        "\n",
        "    results = {}\n",
        "    for cls_id in range(1, num_classes):\n",
        "        preds = class_preds[cls_id]\n",
        "        total_gt = class_gt_counts[cls_id]\n",
        "        if not preds:\n",
        "            results[cls_id] = {'p': 0.0, 'r': 0.0, 'f1': 0.0}\n",
        "            continue\n",
        "\n",
        "        preds.sort(key=lambda x: x[0], reverse=True)\n",
        "        preds_np = np.array(preds)\n",
        "        tp_cumsum = np.cumsum(preds_np[:, 1])\n",
        "        fp_cumsum = np.cumsum(1 - preds_np[:, 1])\n",
        "\n",
        "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-16)\n",
        "        recalls = tp_cumsum / (total_gt + 1e-16)\n",
        "        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-16)\n",
        "\n",
        "        best_idx = np.argmax(f1_scores)\n",
        "        results[cls_id] = {\n",
        "            'p': precisions[best_idx],\n",
        "            'r': recalls[best_idx],\n",
        "            'f1': f1_scores[best_idx]\n",
        "        }\n",
        "    return results"
      ],
      "metadata": {
        "id": "DYyhiWuibTtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device):\n",
        "    model.train()\n",
        "    loss_total = 0\n",
        "    steps = 0\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_total += losses.item()\n",
        "        steps += 1\n",
        "\n",
        "    return loss_total / max(steps, 1)\n",
        "\n",
        "def run_experiment(condition, epochs=100, patience=15):\n",
        "    print(f\"\\nStarting Experiment: {condition} | Patience: {patience}\")\n",
        "\n",
        "    train_transform = get_transforms(condition)\n",
        "    val_transform = get_transforms('Raw')\n",
        "\n",
        "    train_ds = YOLODataset(LOCAL_DATA_DIR, split='train', transforms=train_transform, mapping=TARGET_MAPPING)\n",
        "    val_ds = YOLODataset(LOCAL_DATA_DIR, split='val', transforms=val_transform, mapping=TARGET_MAPPING)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "    model = get_model_mobilenetv3_large(NUM_CLASSES).to(DEVICE)\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.AdamW(params, lr=0.0001, weight_decay=0.0005)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
        "\n",
        "    save_path = os.path.join(BASE_SAVE_DIR, condition)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    history = []\n",
        "    best_map = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = train_one_epoch(model, optimizer, train_loader, DEVICE)\n",
        "        scheduler.step()\n",
        "\n",
        "        map_metrics = evaluate_map_complete(model, val_loader, DEVICE)\n",
        "        f1_metrics = evaluate_best_f1(model, val_loader, DEVICE, NUM_CLASSES)\n",
        "\n",
        "        current_map = map_metrics['map_50']\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        avg_p = np.mean([f1_metrics[c]['p'] for c in f1_metrics])\n",
        "        avg_r = np.mean([f1_metrics[c]['r'] for c in f1_metrics])\n",
        "\n",
        "        log_entry = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'val_map_50': current_map,\n",
        "            'val_map_50_95': map_metrics['map'],\n",
        "            'avg_precision': avg_p,\n",
        "            'avg_recall': avg_r,\n",
        "            'time': duration\n",
        "        }\n",
        "        history.append(log_entry)\n",
        "        pd.DataFrame(history).to_csv(os.path.join(save_path, \"metrics.csv\"), index=False)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss:.4f} | mAP@50: {current_map:.4f} | Time: {duration:.1f}s\")\n",
        "\n",
        "        if current_map > best_map:\n",
        "            best_map = current_map\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), os.path.join(save_path, \"best_model.pth\"))\n",
        "            print(\"  > Model Saved (Best mAP)\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"  > Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return best_map"
      ],
      "metadata": {
        "id": "NlW5VarEcODM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run Baseline (Raw)\n",
        "    map_raw = run_experiment(\"Raw\", epochs=100)\n",
        "\n",
        "    # Run Augmented (Tuned)\n",
        "    map_tuned = run_experiment(\"Tuned\", epochs=100)\n",
        "\n",
        "    # Final Visualization\n",
        "    results = pd.DataFrame([\n",
        "        {'Condition': 'Raw', 'mAP': map_raw},\n",
        "        {'Condition': 'Tuned', 'mAP': map_tuned}\n",
        "    ])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(results['Condition'], results['mAP'], color=['gray', '#d62728'])\n",
        "    plt.title(\"Faster R-CNN Performance: Raw vs. Augmented\")\n",
        "    plt.ylabel(\"mAP@50\")\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f'{yval:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.savefig(os.path.join(BASE_SAVE_DIR, \"final_comparison.png\"))\n",
        "    plt.show()\n",
        "    print(\"Experiment Complete. Results saved.\")"
      ],
      "metadata": {
        "id": "UNzv4tg8cVeQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
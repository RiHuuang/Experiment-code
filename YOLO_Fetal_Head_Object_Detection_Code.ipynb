{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# [INFO] Install necessary packages\n",
        "!pip install -q ultralytics segmentation-models\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import shutil\n",
        "import yaml\n",
        "import random\n",
        "import re\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# [INFO] Reproducibility Setup\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# [INFO] Directory Configuration\n",
        "BASE_SAVE_DIR = \"/content/drive/MyDrive/Model/Final_Experiments_V2\"\n",
        "DRIVE_YAML_PATH = \"/content/drive/MyDrive/Dataset/FINAL_YOLO_SPLIT/dataset.yaml\"\n",
        "DATASET_YAML = os.path.join(LOCAL_DATA_DIR, \"dataset.yaml\")\n",
        "\n",
        "os.makedirs(BASE_SAVE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "rBNW7reE46AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "DATASET_DIR = '/content/drive/MyDrive/Dataset/PROCESSED_YOLO'\n",
        "IMG_DIR = os.path.join(DATASET_DIR, \"images\")\n",
        "LABEL_DIR = os.path.join(DATASET_DIR, \"labels\")\n",
        "\n",
        "# Mapping YOLO ID to Class Name\n",
        "YOLO_ID_TO_NAME = {\n",
        "    0: \"Brain\",\n",
        "    1: \"CSP\",\n",
        "    2: \"LV\"\n",
        "}\n",
        "\n",
        "def get_box_statistics(img_path):\n",
        "    try:\n",
        "        basename = os.path.basename(img_path)\n",
        "        name_no_ext = os.path.splitext(basename)[0]\n",
        "        label_path = os.path.join(LABEL_DIR, name_no_ext + \".txt\")\n",
        "\n",
        "        if not os.path.exists(label_path):\n",
        "            return []\n",
        "\n",
        "        boxes = []\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) < 5: continue\n",
        "\n",
        "                cls_id = int(parts[0])\n",
        "                norm_w = float(parts[3])\n",
        "                norm_h = float(parts[4])\n",
        "                norm_area = norm_w * norm_h  # Normalized Area\n",
        "\n",
        "                boxes.append({\n",
        "                    \"filename\": basename,\n",
        "                    \"class_id\": cls_id,\n",
        "                    \"class_name\": YOLO_ID_TO_NAME.get(cls_id, \"Unknown\"),\n",
        "                    \"width_norm\": norm_w,\n",
        "                    \"height_norm\": norm_h,\n",
        "                    \"area_norm\": norm_area\n",
        "                })\n",
        "        return boxes\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def run_dataset_analysis():\n",
        "    print(\"Starting Bounding Box Analysis (Normalized)\")\n",
        "    image_paths = sorted(glob(os.path.join(IMG_DIR, \"*\")))\n",
        "    workers = multiprocessing.cpu_count()\n",
        "\n",
        "    all_boxes = []\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=workers) as executor:\n",
        "        futures = [executor.submit(get_box_statistics, p) for p in image_paths]\n",
        "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Scanning Labels\"):\n",
        "            res = f.result()\n",
        "            if res:\n",
        "                all_boxes.extend(res)\n",
        "\n",
        "    if not all_boxes:\n",
        "        print(\"No boxes found.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(all_boxes)\n",
        "\n",
        "    # Set style to match reference (white grid)\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "    # --- Visualization 1: Class Distribution ---\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    # Using viridis palette\n",
        "    ax = sns.countplot(data=df, x='class_name', order=['Brain', 'CSP', 'LV'], palette='viridis')\n",
        "\n",
        "    plt.title(\"Class Distribution\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Object Count\")\n",
        "\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{int(p.get_height())}',\n",
        "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='bottom', fontsize=10, color='black', xytext=(0, 2),\n",
        "                    textcoords='offset points')\n",
        "    plt.show()\n",
        "\n",
        "    # --- Visualization 2: Object Size Distribution (Histogram + KDE) ---\n",
        "    # Filter only CSP and LV\n",
        "    df_sub = df[df['class_name'].isin(['CSP', 'LV'])]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.histplot(\n",
        "        data=df_sub,\n",
        "        x='area_norm',\n",
        "        hue='class_name',\n",
        "        kde=True,\n",
        "        element=\"step\",\n",
        "        stat=\"density\",\n",
        "        common_norm=False,\n",
        "        palette={'CSP': '#fc8d62', 'LV': '#66c2a5'}, # Orange & Teal Green\n",
        "        alpha=0.3\n",
        "    )\n",
        "    plt.title(\"Object Size Distribution (CSP vs LV)\")\n",
        "    plt.xlabel(\"Normalized Area (W x H)\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.show()\n",
        "\n",
        "    # --- Visualization 3: Width vs Height Scatter ---\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sns.scatterplot(\n",
        "        data=df_sub,\n",
        "        x='width_norm',\n",
        "        y='height_norm',\n",
        "        hue='class_name',\n",
        "        style='class_name',\n",
        "        alpha=0.6,\n",
        "        palette={'CSP': '#fc8d62', 'LV': '#2b8cbe'} # Orange & Blue/Teal\n",
        "    )\n",
        "\n",
        "    # Diagonal line (Aspect Ratio 1:1)\n",
        "    plt.plot([0, 1], [0, 1], 'r-', alpha=0.4, linewidth=1)\n",
        "\n",
        "    plt.title(\"Box Width vs Height Distribution\")\n",
        "    plt.xlabel(\"Width (Normalized)\")\n",
        "    plt.ylabel(\"Height (Normalized)\")\n",
        "    plt.xlim(0, 1.0)\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.legend(title=\"Class Name\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Print Summary Statistics\n",
        "    print(\"\\n[INFO] Average Normalized Area:\")\n",
        "    print(df.groupby('class_name')['area_norm'].mean().round(4))\n",
        "\n",
        "# Run Analysis\n",
        "run_dataset_analysis()"
      ],
      "metadata": {
        "id": "3nzilRcb49GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_DATA_DIR = \"/content/drive/MyDrive/Dataset/PROCESSED_YOLO\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Dataset/FINAL_YOLO_SPLIT\"\n",
        "SPLIT_RATIOS = (0.70, 0.15, 0.15) # Train, Val, Test\n",
        "CLASS_NAMES = ['Brain', 'CSP', 'LV']\n",
        "\n",
        "def extract_subject_id(filename):\n",
        "    \"\"\"Extracts patient ID from filename to prevent data leakage.\"\"\"\n",
        "    match = re.search(r\"Diverse_(\\d+)_\", filename) or re.search(r\"(Patient\\d+)\", filename)\n",
        "    return match.group(1) if match else filename\n",
        "\n",
        "def copy_file_worker(args):\n",
        "    try:\n",
        "        shutil.copy2(*args)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def split_dataset():\n",
        "    print(f\"[INFO] Initializing dataset split at {OUTPUT_DIR}...\")\n",
        "    img_src = os.path.join(SOURCE_DATA_DIR, \"images\")\n",
        "    lbl_src = os.path.join(SOURCE_DATA_DIR, \"labels\")\n",
        "\n",
        "    if not os.path.exists(img_src):\n",
        "        print(\"[ERROR] Source directory empty.\")\n",
        "        return\n",
        "\n",
        "    # Grouping\n",
        "    subject_map = {}\n",
        "    for f in os.listdir(img_src):\n",
        "        if not f.endswith(('.png', '.jpg', '.jpeg')): continue\n",
        "        subj_id = extract_subject_id(f)\n",
        "        subject_map.setdefault(subj_id, []).append(f)\n",
        "\n",
        "    subjects = list(subject_map.keys())\n",
        "    random.shuffle(subjects) # Deterministic due to set_seed()\n",
        "\n",
        "    n = len(subjects)\n",
        "    n_train = int(n * SPLIT_RATIOS[0])\n",
        "    n_val = int(n * SPLIT_RATIOS[1])\n",
        "\n",
        "    splits = {\n",
        "        'train': subjects[:n_train],\n",
        "        'val': subjects[n_train:n_train+n_val],\n",
        "        'test': subjects[n_train+n_val:]\n",
        "    }\n",
        "\n",
        "    print(f\"[INFO] Split Counts: Train={len(splits['train'])}, Val={len(splits['val'])}, Test={len(splits['test'])}\")\n",
        "\n",
        "    tasks = []\n",
        "    for split_name, subj_list in splits.items():\n",
        "        dst_img = os.path.join(OUTPUT_DIR, split_name, \"images\")\n",
        "        dst_lbl = os.path.join(OUTPUT_DIR, split_name, \"labels\")\n",
        "        os.makedirs(dst_img, exist_ok=True)\n",
        "        os.makedirs(dst_lbl, exist_ok=True)\n",
        "\n",
        "        for subj in subj_list:\n",
        "            for fname in subject_map[subj]:\n",
        "                tasks.append((os.path.join(img_src, fname), os.path.join(dst_img, fname)))\n",
        "                txt_name = os.path.splitext(fname)[0] + \".txt\"\n",
        "                if os.path.exists(os.path.join(lbl_src, txt_name)):\n",
        "                    tasks.append((os.path.join(lbl_src, txt_name), os.path.join(dst_lbl, txt_name)))\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=min(multiprocessing.cpu_count(), 12)) as exc:\n",
        "        list(tqdm(exc.map(copy_file_worker, tasks), total=len(tasks), desc=\"Copying Files\"))\n",
        "\n",
        "    # Generate YAML\n",
        "    yaml_data = {\n",
        "        'path': OUTPUT_DIR,\n",
        "        'train': 'train/images',\n",
        "        'val': 'val/images',\n",
        "        'test': 'test/images',\n",
        "        'names': dict(enumerate(CLASS_NAMES))\n",
        "    }\n",
        "    with open(os.path.join(OUTPUT_DIR, 'dataset.yaml'), 'w') as f:\n",
        "        yaml.dump(yaml_data, f, sort_keys=False)\n",
        "\n",
        "    print(f\"[INFO] Dataset split complete.\")\n",
        "\n",
        "if not os.path.exists(os.path.join(OUTPUT_DIR, 'dataset.yaml')):\n",
        "    split_dataset()\n",
        "else:\n",
        "    print(\"[INFO] Dataset split already exists.\")"
      ],
      "metadata": {
        "id": "BuLxrX5m4_Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 0 if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"[INFO] Computation Device: {DEVICE}\")\n",
        "\n",
        "COMMON_TRAIN_ARGS = {\n",
        "    'imgsz': 640,\n",
        "    'epochs': 100,\n",
        "    'patience': 15,\n",
        "    'batch': 16,\n",
        "    'workers': 4,\n",
        "    'project': BASE_SAVE_DIR,\n",
        "    'device': DEVICE,\n",
        "    'verbose': False,\n",
        "    'exist_ok': True,\n",
        "    'optimizer': 'AdamW',\n",
        "    'lr0': 0.001,\n",
        "    'lrf': 0.00001,\n",
        "    'cos_lr': True,\n",
        "    'cache': True\n",
        "}\n",
        "\n",
        "# Augmentation Profile\n",
        "AUG_NAME = \"Aug_Profile_Tuned\"\n",
        "AUG_PARAMS = {\n",
        "    'augment': True,\n",
        "    'degrees': 45.0,\n",
        "    'shear': 5.0,\n",
        "    'perspective': 0.0005,\n",
        "    'translate': 0.2,\n",
        "    'scale': 0.6,\n",
        "    'fliplr': 0.5,\n",
        "    'flipud': 0.0\n",
        "}\n",
        "\n",
        "MODELS = [\n",
        "    'yolov5su.pt', 'yolov5mu.pt',\n",
        "    'yolov8s.pt',  'yolov8m.pt',\n",
        "    'yolo11s.pt',  'yolo11m.pt',\n",
        "    'yolo12s.pt',  'yolo12m.pt'\n",
        "]"
      ],
      "metadata": {
        "id": "UGqBexZq5AUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_map50_from_csv(run_folder):\n",
        "    \"\"\"Parses training CSV log to retrieve best mAP@50.\"\"\"\n",
        "    csv_path = os.path.join(run_folder, 'results.csv')\n",
        "    if not os.path.exists(csv_path): return 0.0\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    col = 'metrics/mAP50(B)'\n",
        "    return df[col].max() if col in df.columns else 0.0\n",
        "\n",
        "def train_wrapper(run_name, model_weights, params):\n",
        "    \"\"\"Wraps the YOLO training process.\"\"\"\n",
        "    print(f\"[INFO] Training: {run_name}\")\n",
        "    args = COMMON_TRAIN_ARGS.copy()\n",
        "    args.update(params)\n",
        "    args['name'] = run_name\n",
        "    args['data'] = DATASET_YAML\n",
        "\n",
        "    try:\n",
        "        model = YOLO(model_weights)\n",
        "        start = time.time()\n",
        "        model.train(**args)\n",
        "        duration = time.time() - start\n",
        "\n",
        "        run_dir = os.path.join(BASE_SAVE_DIR, run_name)\n",
        "        score = get_map50_from_csv(run_dir)\n",
        "\n",
        "        # Cleanup to save VRAM\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return score, run_dir, duration\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Training failed for {run_name}: {e}\")\n",
        "        return 0.0, None, 0.0"
      ],
      "metadata": {
        "id": "hF68ckeM5Vmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark(aug_params, aug_name):\n",
        "    print(f\"\\n[INFO] Starting Benchmark: Baseline vs. {aug_name}\")\n",
        "    results = []\n",
        "\n",
        "    # Baseline Parameters (Optimizer auto, augmentation disabled)\n",
        "    raw_params = {'augment': False, 'optimizer': 'auto'}\n",
        "\n",
        "    for model_file in MODELS:\n",
        "        model_name = model_file.replace('.pt', '')\n",
        "\n",
        "        # 1. Baseline Run\n",
        "        run_name_raw = f\"Benchmark_{model_name}_Raw\"\n",
        "        score_raw, _, time_raw = train_wrapper(run_name_raw, model_file, raw_params)\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_name,\n",
        "            'Size': model_name[-1].upper(),\n",
        "            'Version': model_name[:6],\n",
        "            'Condition': 'Baseline',\n",
        "            'mAP50': score_raw,\n",
        "            'Time_Min': time_raw / 60\n",
        "        })\n",
        "\n",
        "        # 2. Augmented Run\n",
        "        run_name_aug = f\"Benchmark_{model_name}_{aug_name}\"\n",
        "        score_aug, _, time_aug = train_wrapper(run_name_aug, model_file, aug_params)\n",
        "\n",
        "        results.append({\n",
        "            'Model': model_name,\n",
        "            'Size': model_name[-1].upper(),\n",
        "            'Version': model_name[:6],\n",
        "            'Condition': 'Augmented',\n",
        "            'mAP50': score_aug,\n",
        "            'Time_Min': time_aug / 60\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "Y7viNQ9V5YaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(df):\n",
        "    if df.empty: return\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    df['Display_Name'] = df['Version'] + '-' + df['Size']\n",
        "\n",
        "    sns.barplot(\n",
        "        data=df,\n",
        "        x='Display_Name',\n",
        "        y='mAP50',\n",
        "        hue='Condition',\n",
        "        palette={'Baseline': 'gray', 'Augmented': 'firebrick'}\n",
        "    )\n",
        "\n",
        "    plt.title(\"Impact of Data Augmentation on Detection Performance\")\n",
        "    plt.ylabel(\"mAP@50\")\n",
        "    plt.xlabel(\"Model Variant\")\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    save_path = os.path.join(BASE_SAVE_DIR, \"Benchmark_Comparison.png\")\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"[INFO] Plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_results = run_benchmark(AUG_PARAMS, AUG_NAME)\n",
        "\n",
        "    csv_path = os.path.join(BASE_SAVE_DIR, \"final_benchmark_metrics.csv\")\n",
        "    df_results.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(\"\\n[INFO] Final Results:\")\n",
        "    print(df_results.sort_values(by='mAP50', ascending=False))\n",
        "\n",
        "    plot_results(df_results)"
      ],
      "metadata": {
        "id": "39tBEE0N5ZXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}